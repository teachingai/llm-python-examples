{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dmeta-embedding 是一款跨领域、跨任务、开箱即用的中文 Embedding 模型，适用于搜索、问答、智能客服、LLM+RAG 等各种业务场景，支持使用 Transformers/Sentence-Transformers/Langchain 等工具加载推理。\n",
    "\n",
    "Huggingface：https://huggingface.co/DMetaSoul/Dmeta-embedding-zh\n",
    "\n",
    "优势特点如下：\n",
    "\n",
    "- 多任务、场景泛化性能优异，目前已取得 MTEB 中文榜单第二成绩（2024.01.25）\n",
    "- 模型参数大小仅 400MB，对比参数量超过 GB 级模型，可以极大降低推理成本\n",
    "- 支持上下文窗口长度达到 1024，对于长文本检索、RAG 等场景更适配\n",
    "\n",
    "## Usage\n",
    "\n",
    "目前模型支持通过 Sentence-Transformers, Langchain, Huggingface Transformers 等主流框架进行推理，具体用法参考各个框架的示例。\n",
    "Sentence-Transformers \n",
    "\n",
    "### Dmeta-embedding 模型支持通过 sentence-transformers 来加载推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "texts1 = [\"胡子长得太快怎么办？\", \"在香港哪里买手表好\"]\n",
    "texts2 = [\"胡子长得快怎么办？\", \"怎样使胡子不浓密！\", \"香港买手表哪里好\", \"在杭州手机到哪里买\"]\n",
    "\n",
    "model = SentenceTransformer('DMetaSoul/Dmeta-embedding')\n",
    "embs1 = model.encode(texts1, normalize_embeddings=True)\n",
    "embs2 = model.encode(texts2, normalize_embeddings=True)\n",
    "\n",
    "# 计算两两相似度\n",
    "similarity = embs1 @ embs2.T\n",
    "print(similarity)\n",
    "\n",
    "# 获取 texts1[i] 对应的最相似 texts2[j]\n",
    "for i in range(len(texts1)):\n",
    "    scores = []\n",
    "    for j in range(len(texts2)):\n",
    "        scores.append([texts2[j], similarity[i][j]])\n",
    "    scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    print(f\"查询文本：{texts1[i]}\")\n",
    "    for text2, score in scores:\n",
    "        print(f\"相似文本：{text2}，打分：{score}\")\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Dmeta-embedding 模型支持通过 LLM 工具框架 langchain 来加载推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"DMetaSoul/Dmeta-embedding\"\n",
    "model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "model = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "\n",
    "texts1 = [\"胡子长得太快怎么办？\", \"在香港哪里买手表好\"]\n",
    "texts2 = [\"胡子长得快怎么办？\", \"怎样使胡子不浓密！\", \"香港买手表哪里好\", \"在杭州手机到哪里买\"]\n",
    "\n",
    "embs1 = model.embed_documents(texts1)\n",
    "embs2 = model.embed_documents(texts2)\n",
    "embs1, embs2 = np.array(embs1), np.array(embs2)\n",
    "\n",
    "# 计算两两相似度\n",
    "similarity = embs1 @ embs2.T\n",
    "print(similarity)\n",
    "\n",
    "# 获取 texts1[i] 对应的最相似 texts2[j]\n",
    "for i in range(len(texts1)):\n",
    "    scores = []\n",
    "    for j in range(len(texts2)):\n",
    "        scores.append([texts2[j], similarity[i][j]])\n",
    "    scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    print(f\"查询文本：{texts1[i]}\")\n",
    "    for text2, score in scores:\n",
    "        print(f\"相似文本：{text2}，打分：{score}\")\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Transformers\n",
    "\n",
    "Dmeta-embedding 模型支持通过 HuggingFace Transformers 框架来加载推理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def cls_pooling(model_output):\n",
    "    return model_output[0][:, 0]\n",
    "\n",
    "\n",
    "texts1 = [\"胡子长得太快怎么办？\", \"在香港哪里买手表好\"]\n",
    "texts2 = [\"胡子长得快怎么办？\", \"怎样使胡子不浓密！\", \"香港买手表哪里好\", \"在杭州手机到哪里买\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('DMetaSoul/Dmeta-embedding')\n",
    "model = AutoModel.from_pretrained('DMetaSoul/Dmeta-embedding')\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs1 = tokenizer(texts1, padding=True, truncation=True, return_tensors='pt')\n",
    "    inputs2 = tokenizer(texts2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    model_output1 = model(**inputs1)\n",
    "    model_output2 = model(**inputs2)\n",
    "    embs1, embs2 = cls_pooling(model_output1), cls_pooling(model_output2)\n",
    "    embs1 = torch.nn.functional.normalize(embs1, p=2, dim=1).numpy()\n",
    "    embs2 = torch.nn.functional.normalize(embs2, p=2, dim=1).numpy()\n",
    "\n",
    "# 计算两两相似度\n",
    "similarity = embs1 @ embs2.T\n",
    "print(similarity)\n",
    "\n",
    "# 获取 texts1[i] 对应的最相似 texts2[j]\n",
    "for i in range(len(texts1)):\n",
    "    scores = []\n",
    "    for j in range(len(texts2)):\n",
    "        scores.append([texts2[j], similarity[i][j]])\n",
    "    scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    print(f\"查询文本：{texts1[i]}\")\n",
    "    for text2, score in scores:\n",
    "        print(f\"相似文本：{text2}，打分：{score}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
